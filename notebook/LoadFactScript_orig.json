{
	"name": "LoadFactScript_orig",
	"properties": {
		"folder": {
			"name": "Lakehouse/Scripts"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "d7ee03ce-e6d4-46db-945f-fa5235f16373"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Load Fact\r\n",
					"This notebook takes a Spark Temp Table and a few parameters and dynamically updates a Fact in the Serverless SQL database (Gold layer)."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Example Parameters:\r\n",
					"#databaseName = 'lakehouse'\r\n",
					"#schemaName = \"Fact\"\r\n",
					"#tableName = \"Encounter\"\r\n",
					"#surrogateKey = \"EncounterSK\"\r\n",
					"#naturalKeyColumnList = [\"PersonUid\"]\r\n",
					"#isIncrementalLoad = True\r\n",
					"#incrementalColumn = \"EncounterDateNK\""
				],
				"execution_count": 37
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"sparkTableName = f\"{schemaName}{tableName}\"\r\n",
					"\r\n",
					"if \"maxDate\" not in locals() and \"maxDate\" not in globals():\r\n",
					"    maxDate = '1900-01-01'"
				],
				"execution_count": 38
			},
			{
				"cell_type": "code",
				"source": [
					"# Assign additional variables\r\n",
					"targetContainer = 'datalake'\r\n",
					"storageAccountName = mssparkutils.credentials.getSecretWithLS(\"ls_keyvault\", \"kv-datalake-storage-account-name\")\r\n",
					"targetDirectory = f'{databaseName}/{schemaName}/{tableName}'\r\n",
					"targetTableFullPath = 'abfss://' + targetContainer + \"@\" + storageAccountName + \".dfs.core.windows.net/\" + targetDirectory\r\n",
					"metadataColumns = [\"ETLRowValueHash\", \"ETLKeyHash\", \"ETLRowInsertedDatetime\", \"ETLRowUpdatedDatetime\"]\r\n",
					"metadataColumns.extend(naturalKeyColumnList)\r\n",
					"fullTableName = schemaName + tableName\r\n",
					"stagingTableName = 'staging_' + schemaName + tableName"
				],
				"execution_count": 39
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Generate hash values for each row\r\n",
					"# Add ETL Metadata Columns for each row. \r\n",
					"from datetime import datetime\r\n",
					"from pyspark.sql.functions import col\r\n",
					"from delta.tables import *\r\n",
					"\r\n",
					"timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\r\n",
					"\r\n",
					"sourceDf = spark.table(sparkTableName)\r\n",
					"exists = DeltaTable.isDeltaTable(spark, targetTableFullPath)\r\n",
					"hashColumns = [x for x in sourceDf.columns if x not in metadataColumns]\r\n",
					"\r\n",
					"sourceDf = sourceDf \\\r\n",
					"    .withColumn(\"ETLKeyHash\",generateHash(naturalKeyColumnList)) \\\r\n",
					"    .withColumn(\"ETLRowValueHash\", generateHash(hashColumns)) \\\r\n",
					"    .withColumn(\"ETLRowInsertedDatetime\", lit(timestamp)) \\\r\n",
					"    .withColumn(\"ETLRowUpdatedDatetime\", lit(timestamp))\r\n",
					"    "
				],
				"execution_count": 41
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Read target table, add Surrogate Key to Source Dataframe.\r\n",
					"allColumns = [x for x in sourceDf.columns]\r\n",
					"\r\n",
					"# If table exists, get surrogate key values for existing rows and generate surrogate key value for new rows\r\n",
					"if exists:\r\n",
					"    # If source table is incremental, only select rows which have been updated/new inserts.\r\n",
					"    if isIncrementalLoad:\r\n",
					"        targetDf = spark.sql(f\"SELECT * FROM {schemaName}.{tableName} WHERE {incrementalColumn} >= '{maxDate}'\")\r\n",
					"\r\n",
					"    else:\r\n",
					"        targetDf = spark.read.format(\"delta\").load(targetTableFullPath)\r\n",
					"\r\n",
					"    targetDf.createOrReplaceTempView('targetDf')   \r\n",
					"    maxSK = spark.sql(f\"SELECT MAX({surrogateKey}) AS LastSK FROM targetDf\").first()[0]\r\n",
					"    sourceDf = sourceDf.join(targetDf, how='left_anti', on=['ETLRowValueHash', 'ETLKeyHash'])\r\n",
					"    sourceDf = sourceDf.alias('src').join(targetDf.alias('tgt'), on='ETLKeyHash', how='left').select(col(f\"tgt.{surrogateKey}\"), 'src.*')\r\n",
					"    sourceDf = sourceDf.withColumn(surrogateKey, coalesce(surrogateKey, expr(f'ROW_NUMBER() OVER (ORDER BY ETLKeyHash) + {maxSK}')))\r\n",
					"    sourceDf = sourceDf.select(col(surrogateKey), *allColumns)\r\n",
					"\r\n",
					"# If table does not exist, generate surrogate key values for every row\r\n",
					"else:\r\n",
					"    sourceDf.createOrReplaceTempView(\"sourceTemp\")\r\n",
					"    sourceDf = spark.sql(f\"SELECT *, ROW_NUMBER() OVER (ORDER BY ETLKeyHash) AS {surrogateKey} FROM sourceTemp\")\r\n",
					"    sourceDf = sourceDf.select(col(surrogateKey), *allColumns)\r\n",
					"    \r\n",
					"sourceDf.createOrReplaceTempView(stagingTableName)"
				],
				"execution_count": 42
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Setup the update field list to include all columns except ETLRowInsertedDatetime\r\n",
					"updateTargetColumns = [x for x in sourceDf.columns if x not in ['ETLRowInsertedDatetime']]\r\n",
					"updateSourceColumns = [f\"src.{x}\" for x in sourceDf.columns if x not in ['ETLRowInsertedDatetime']]\r\n",
					"updateDictionary = dict(zip(updateTargetColumns, updateSourceColumns))"
				],
				"execution_count": 43
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"createQuery = \"\"\"CREATE TABLE IF NOT EXISTS {3}.{0} \r\n",
					"USING DELTA\r\n",
					"LOCATION '{1}'\r\n",
					"TBLPROPERTIES(delta.enableChangeDataFeed = True)\r\n",
					"AS\r\n",
					"SELECT * FROM {2} WHERE 1 = 0\"\"\".format(fullTableName, targetTableFullPath, stagingTableName, databaseName)\r\n",
					"\r\n",
					"spark.sql(createQuery)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Merge into target table\r\n",
					"targetTable = DeltaTable.forPath(spark, targetTableFullPath)\r\n",
					"\r\n",
					"(\r\n",
					"    targetTable.alias('tgt') \r\n",
					"        .merge( \r\n",
					"            sourceDf.alias('src'),\r\n",
					"            'tgt.ETLKeyHash = src.ETLKeyHash and tgt.ETLRowValueHash != src.ETLRowValueHash' \r\n",
					"        ) \r\n",
					"        .whenMatchedUpdate(set = updateDictionary\r\n",
					"        )\r\n",
					"        .whenNotMatchedInsertAll()\r\n",
					"        .execute()\r\n",
					")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#### The below code supports creating a serverless view on top of the delta table. This can be utilized in the future to support row-level security."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Create a Serverless View on top of the Delta Table (requires database and external data source to exist)\r\n",
					"#createSchemaQuery = f\"IF NOT EXISTS (SELECT * FROM sys.schemas WHERE name = '{schemaName}') EXEC('CREATE SCHEMA {schemaName}');\"\r\n",
					"#print(createSchemaQuery)\r\n",
					"#execute_serverless_command(createSchemaQuery)\r\n",
					"\r\n",
					"# Using 'datalake_msi' datasource which uses the Managed Identity credential. Use 'datalake' for passthrough authentication.\r\n",
					"#createViewQuery = f\"CREATE OR ALTER VIEW {schemaName}.{tableName} AS SELECT * FROM OPENROWSET(BULK '/{targetDirectory}', DATA_SOURCE = 'datalake_msi', FORMAT = 'DELTA') AS [{tableName}];\"\r\n",
					"#print(createViewQuery)\r\n",
					"#execute_serverless_command(createViewQuery)"
				],
				"execution_count": 85
			}
		]
	}
}